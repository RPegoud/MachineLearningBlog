[{"content":"Coming soon !\n1 2 3 import jax.numpy as jnp from jax import random, tree_map, vmap from jax.scipy.special import logsumexp 1 2 3 4 5 6 LAYER_SIZES = [784, 512, 512, 10] LEARNING_RATE = 1e-2 N_EPOCHS = 8 BATCH_SIZE = 128 N_CLASSES = 10 RANDOM_SEED = 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def init_weights_biases( in_size: int, out_size: int, random_key: random.PRNGKey, scale: float = 1e-3 ) -\u0026gt; tuple[jnp.array, jnp.array]: \u0026#34;\u0026#34;\u0026#34; Initialize weights and biases for a neural network layer. Args: in_size (int): Number of input features. out_size (int): Number of output neurons. random_key (random.PRNGKey): Random key for reproducible random numbers. scale (float, optional): Scaling factor for weight initialization. Defaults to 1e-3. Returns: Tuple[jnp.ndarray, jnp.ndarray]: A tuple containing initialized weights and biases. \u0026#34;\u0026#34;\u0026#34; w_key, b_key = random.split(random_key) return scale * random.normal(w_key, (out_size, in_size)), scale * random.normal( b_key, (out_size,) ) def random_layer_init(layer_sizes: list, random_key: random.PRNGKey) -\u0026gt; list[tuple[jnp.ndarray, jnp.ndarray]]: \u0026#34;\u0026#34;\u0026#34; Initialize weights and biases for all layers in a neural network. Args: layer_sizes (List[int]): List of layer sizes, including input and output dimensions. random_key (random.PRNGKey): Random key for reproducible random numbers. Returns: List[Tuple[jnp.ndarray, jnp.ndarray]]: A list of tuples, each containing initialized weights and biases for a layer. \u0026#34;\u0026#34;\u0026#34; keys = random.split(random_key, len(layer_sizes)) return [ init_weights_biases(in_size, out_size, key) for in_size, out_size, key in zip(layer_sizes[:-1], layer_sizes[1:], keys) ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def relu(x): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; return jnp.maximum(0, x) def predict(params, image): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; # initialize the activations activations = image for w, b in params[:-1]: outputs = jnp.dot(w, activations) + b activations = relu(outputs) final_w, final_b = params[-1] logits = jnp.dot(final_w, activations) + final_b return logits - logsumexp(logits) 1 2 3 def loss(params: list, images: jnp.array, targets: jnp.array) -\u0026gt; float: preds = batched_predict(params, images) return -jnp.mean(preds * targets) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 if __name__ == \u0026#34;__main__\u0026#34;: key = random.PRNGKey(RANDOM_SEED) params = random_layer_init(LAYER_SIZES, key) print(tree_map(lambda x: x.shape, params)) batched_predict = vmap(predict, in_axes=(None, 0)) random_flattened_image = random.normal( random.PRNGKey(1), ( 10, 28 * 28, ), ) preds = batched_predict(params, random_flattened_image) print(preds.shape) ","date":"2023-09-12T00:00:00Z","image":"https://machine-learning-blog.vercel.app/p/jax_mlp/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1256981_120x120_fill_q75_box_smart1.jpg","permalink":"https://machine-learning-blog.vercel.app/p/jax_mlp/","title":"Building a Neural Network with JAX: An Illustrated Guide"},{"content":"Markov Decision Processes are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.\nYou might have already wondered how Reinforcement Learning (RL) agents manage to reach super-human levels in games such as chess. Just like in chess, where a player has to make decisions based on the current state of the board and the available moves, RL agents also face similar decision-making challenges in dynamic environments.\nMarkov Decision Processes (MDP) provide a mathematical framework to model such sequential decision-making problems, making them a fundamental tool in Reinforcement Learning.\nMarkov State To understand an RL problem, we need to define its key components. The main character in this scenario, responsible for learning and making decisions, is referred to as the agent. On the other side of the equation, we have the environment, which encompasses everything the agent interacts with.\nThese two, the agent and the environment, engage in an ongoing interaction. The agent takes actions, and the environment responds to these actions, presenting new situations to the agent. After each action of the agent, the environment returns a state, a snapshot of the environment at a given time, providing essential information for decision-making.\nThe totality of the state is not always revealed to the agent. In games like poker for example, the state could be represented as the cards held by all the players, the cards on the table and the order of cards in the deck. However, each player only has partial knowledge of the state. While their action modify the state of the environment, they only get an observation back from the environment. We call these settings partially observable.\nInversely, in chess, each player can observe the state of the board fully.\nFinally, the environment offers rewards, which are specific numerical values that the agent aims to maximize over time by making thoughtful choices in its actions.\nIn a finite MDP, the sets of states, actions and rewards $(\\mathcal{S, A} \\text{ and } \\mathcal{R})$ have a finite number of elements. In this case, the random variables $R_t$ and $S_t$ have discrete probability distributions dependent only on the preceding state and action. We then note the probability of transitioning to a new state $s\u0026rsquo;$ while receiving a reward $r$ based on the current state $s$ and the action $a$ taken by the agent:\n$$p(s\u0026rsquo;,r|s,a) = Pr(S_t=s\u0026rsquo;, R_t=r | S_{t-1}=s, A_{t-1}=a)$$\nFor every value of these random variables, $s\u0026rsquo; \\in \\mathcal S$ and $r \\in \\mathcal R$, there is a probability of those values occuring at time $t$, given particular values of the preceding state and action. The function $p$ defines the dynamics of the MDP. The dynamics function $p : \\mathcal S ~X~ \\mathcal R ~X~ \\mathcal S ~X~ \\mathcal A → [0,1]$ is a probability distribution (as indicated by the \u0026lsquo;$|$\u0026rsquo;), therefore the sum of probabilites of all states $s\u0026rsquo;$ and rewards $r$ given a state $s$ and an action $a$ equals 1. $$\\sum_{s\u0026rsquo; \\in \\mathcal S}\\sum_{r \\in \\mathcal R}p(s\u0026rsquo;,r|s,a)=1$$ In an MDP, the probability given by $p$ completely characterizes the environment dynamics.\nIndeed, an environment having the Markov property means that the probability of each possible value for $\\mathcal S_t$ and $\\mathcal R_t$ depends only on the immediately preceding state and action, $\\mathcal S_{t-1}$ and $\\mathcal A_{t-1}$, and not at all on earlier states and actions.\nThis is best viewed as a restriction not on the decision process, but on the state. The state must include information about all aspects of the past agent–environment interactions that make a difference for the future. This is a crucial assumption in Reinforcement Learning as it simplifies the learning problem by allowing the agent to ignore the history of past states and actions and focus solely on the current state.\nA chess game is a good example of an MDP. We can define the environment\u0026rsquo;s state as the position of all the pieces on the board. In this setting, the game history (sequence of moves played to get to the current position) is not useful to predict the best possible move. Therefore the environment is said to be Markov.\nReward and reward hypothesis: At each timestep, the agent receives a reward $R_t \\in \\mathbb{R}$ defining its purpose. The goal of the agent is to maximize the total amount of rewards i.e. the cumulative reward. This idea is illustrated by the reward hypothesis:\n\u0026ldquo;All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)\u0026rdquo;\nTo complete the chess example, we could define the reward as being positive when the agent wins the game and negative when it loses or draws. One could argue that capturing pieces should generate positive rewards, however it is possible to lose or draw the game after capturing almost all the opponent\u0026rsquo;s pieces.\nIt is important to understand that rewards are used to set the goal to achieve and not the way to achieve it which is for the agent to figure out.\nReturn and episodes Now that we have defined the notion of reward, we are interested in maximizing this reward over time. For this we need to define a new variable, the return.\nThe return $G_t$ is the sum of future rewards at timestep $t$. $$G_t = R_{t+1}+R_{t+2}+R_{t+3}+\u0026hellip;=\\sum_{k=0}^\\infty R_{t+k+1}$$ Importantly, the return is a random variable, as the dynamics of the MDP can be stochastic (i.e. involve randomness). In other words, the same actions can lead to different rewards if the environment dynamics are random. Therefore we want to consider and maximize the expected return, the expected sum of future rewards:\n$$\\mathbb{E}[G_t] = \\mathbb{E}[\\sum_{k=0}^\\infty R_{t+k+1}]$$\nFor this definition to make sense, the sum of rewards has to be finite. What happens if we want the agent to perform a task continuously, i.e. if the number of timesteps is infinite ?\nWe must distinguish two cases:\nEpisodic MDPs: the MDP can be naturally decomposed in episodes, a finite sequence of actions that end on a terminal state. Each episode starts in the same configuration and is independent from previous episodes. A game of chess is again a good example, each game starts in the same setting regardless of previous games. A game ends by a draw or by checkmate (terminal states). When the game ends we can reset the board and start anew. Continuous MDPs: here the number of timesteps in infinite, the MDP goes on continually An example of continuous MDP could be controlling a dam to optimize the energy production depending on the predicted demand. The dam being permanently active, there is no terminal state (if we leave out special events such as maintenance). Continuing tasks and discounting rewards One might wonder, how can the expected return be finite when the MDP goes on forever ?\nFor continuous MDPs, the return is defined as the discounted sum of future rewards. The discount factor $\\gamma \\in [0,1)$ makes sure that rewards far in the future receive a lower weight: $$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \u0026hellip; = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$$\nThis acknowledges that rewards obtained in the future are generally considered less valuable than immediate rewards. This concept is crucial when dealing with tasks that have uncertain, long-term consequences.\nA high discount factor values immediate rewards more, while a low factor assigns similar importance to rewards regardless of when they are received.\nWe can prove that the discounted return is finite by defining $R_{max}$ as the highest reward the agent can receive, therefore: $$G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\leq \\sum_{k=0}^\\infty \\gamma^k R_{max}$$ As $R_{max}$ is constant, we can write: $$\\sum_{k=0}^\\infty \\gamma^k R_{max} = R_{max} \\sum_{k=0}^\\infty \\gamma^k$$ As $\\sum_{k=0}^\\infty \\gamma^k$ is a geometric series converging for $\\gamma \\lt 1$ we can conclude that: $$R_{max} \\sum_{k=0}^\\infty \\gamma^k = R_{max} \\times { {1}\\over{1-\\gamma} } $$ In conclusion, $G_t$ has a finite upper bound and is therefore finite: $$G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\leq R_{max} \\times { {1}\\over{1-\\gamma} }$$\nWrapping up There we have it ! In this article, we learned about MDPs, rewards and how to define the return for episodic and continuing tasks. In the next article, we\u0026rsquo;ll see how we can solve an MDP to find the best possible action in each state by introducing the notions of policy, value functions and last but not least, the Bellman Equation\n","date":"2023-04-03T00:00:00Z","image":"https://machine-learning-blog.vercel.app/p/markov_decision_processes/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_209593_120x120_fill_q75_box_smart1.jpg","permalink":"https://machine-learning-blog.vercel.app/p/markov_decision_processes/","title":"Markov Decision Processes"},{"content":"In the previous articles, we had a look at the general setting of an RL problem by understanding the importance of the Markov property, the notion of rewards and return. Now that the agent evolves in a well defined environment and can receive rewards, we want to find a way to solve the MDP, in other words to maximize the expected return.\nPolicy: To maximize the return, we need our agent to pick the best possible actions from every state. You might wonder, how does the agent select actions ? The answer is pretty straightforward, the agent follows a policy $\\pi$, which is a probability distribution over all actions in a given state. Simply put, the policy is a function that assigns a probability to each available action at a specific point in time. The probability of picking an action $a$ in state $s$ is given by: $$\\pi(a|s)$$ As $\\pi$ is a probability distribution, the sum of all action probabilities is 1: $$\\sum_{a \\in \\mathcal{A(s)} }\\pi(a|s) = 1$$ Finding the best policy is equivalent to maximizing the expected return, therefore, we need a way to determine how good it is to follow a specific policy.\nValue functions: Value functions allow us to get an idea of how valuable it is to pursue a specific policy in a certain state. Here, we\u0026rsquo;ll introduce the state value function and the action value function:\nState value function: The state value function helps to answer the questions:\nHow good is it to be in state $s$ and to follow the policy $\\pi$ thereafter ?\nIn an MDP, the state value function is defined as follows: $$v_\\pi(s) = E_\\pi[G_t|S_t=s]$$ As you can see, the state value function is the expected return given that we are in state $s$ and follow the policy $\\pi$. We can use the definition of the return to obtain the following form: $$v_\\pi(s) = E_\\pi[\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}|S_t=s]$$\nction value function: On the other hand, the action value function answers :\nHow good is it to be in state $s$, pick action $a$ and follow the policy $\\pi$ thereafter ?\nIn an MDP, the action value function is defined as follows: $$q_\\pi(s) = E_\\pi[G_t|S_t=s, A_t = a]$$ This equation is highly similar to the state value function, the only difference is that, before following $\\pi$, we pick an action $a$. $$q_\\pi(s) = E_\\pi[\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}|S_t=s, A_t=a]$$\nBellman equations: Now that we have briefly defined value functions, we will introduce the Bellman equations, that are directly derived from the state value and action value functions. These equations are used to link the value of each state, or state-action pair to the value of its possible successors.\nThe V function: To solve an MDP, we need to evaluate the values of states based on the actions and subsequent states available. The Bellman equation allows us to define the state value function using recursion, which will come in handy and allow us to iterate through the state space. Let\u0026rsquo;s start from the definition of the state value function: $$v_\\pi(s) = E_\\pi[G_t|S_t=s]$$ Using the recursive definition of $G_t$, we obtain: $$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s]$$ We\u0026rsquo;ll now decompose the expectation as three sums, one over each available actions $a$, over each subsequent state $s\u0026rsquo;$ and rewards $r$ (look at the following backup graph for a more visual explanation !): $$v_\\pi(s) = \\sum_a\\pi(a|s)\\sum_{s\u0026rsquo;,r}p[s\u0026rsquo;,r|s,a](r + \\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s\u0026rsquo;)]$$ Here you might recognize that $E_{\\pi}[G_{t+1}|S_{t+1}=s\u0026rsquo;]$ is the value of the state s\u0026rsquo;, therefore: $$v_\\pi(s) = \\sum_a\\pi(a|s)\\sum_{s\u0026rsquo;,r}p[s\u0026rsquo;,r|s,a](r + \\gamma v_\\pi(s\u0026rsquo;))$$ For all state $s \\in \\mathcal{S}$\nNote that $\\sum_{s\u0026rsquo;,r}$ is a compact notation for $\\sum_{s\u0026rsquo;}\\sum_r$\nVisual explanation:\nThis might seem complicated, but no worries, let\u0026rsquo;s look at a more visual explanation with this backup diagram. We start in a state $s$ and want to estimate the value of being in this state. From there we can pick different actions, each action has a probability of being picked depending on the policy $\\pi$ (remember that the policy is a probability distribution). After picking an action, the environment will return a new state $s\u0026rsquo;$ and a reward $r$, both are subject to the environment\u0026rsquo;s dynamics $p$ which is also a probability distribution. The Bellman equation states that the value of being in state $s$ is the sum of the rewards and subsequent state values we can get from picking an action $a$ while being in state $s$ : $$[r + \\gamma v_\\pi(s\u0026rsquo;)]$$This sum is weighted by the probability of obtaining each action, reward and state triplet: $$ \\sum_a\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)$$ Putting it all together, we obtain the Bellman equation for the state value.\nThe Q function: In the same fashion, we can derive the Bellman version of the action-value function starting from the definition: $$q_\\pi(s) = E_\\pi[G_t|S_t=s, A_t=a]$$ However this time, we start from a state $s$ and take an action $a$ (refer to the following backup diagram). As the action we chose is fixed, the policy is not yet intervening. Using the same recursive property og $G_t$, we obtain: $$q_\\pi(s) = \\sum_{s\u0026rsquo;,r} p[s\u0026rsquo;,r|s,a](r + \\gamma E_\\pi[G_{t+1}|S_{t+1}=s\u0026rsquo;)]$$ We now have the expected return for states $s\u0026rsquo;$, however we are interested in the value of state-action pairs. We can therefore use the following transformation and introduce the new action $a\u0026rsquo;$ and its probability defined by $\\pi$: $$\\gamma E_\\pi[G_{t+1}|S_{t+1}=s\u0026rsquo;] = \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;|s\u0026rsquo;)E_\\pi[G_{t+1}|S_{t+1}=s\u0026rsquo;, A_{t+1}=a\u0026rsquo;]$$ We effectively transformed the state value of $s\u0026rsquo;$ into the action value of $s\u0026rsquo;$, following the definition of $q_{\\pi}$, the whole expression becomes: $$q_\\pi(s) = \\sum_{s\u0026rsquo;,r} p[s\u0026rsquo;,r|s,a](r + \\gamma \\sum_{a\u0026rsquo;} \\pi(a\u0026rsquo;,s\u0026rsquo;) q_\\pi(a\u0026rsquo;,s\u0026rsquo;))$$\nVisual explanation:\nThis time, we start from a state $s$ and pick an action $a$, from there we can transition to different states $s\u0026rsquo;$ and receive a reward $r$ depending on the environment\u0026rsquo;s dynamics. As we evaluate all the possible transitions, we get the term $\\sum_{s\u0026rsquo;,r} p(s\u0026rsquo;,r|s,a)$. We then want to estimate the value of the new state $s\u0026rsquo;$ we landed in. This value corresponds to the transition reward $r$ summed with the (discounted) expected return from state $s\u0026rsquo;$. The expected return from state $s\u0026rsquo;$ can be seen as the weighted sum of the values of each state-action pairs, with the weight being the probability of picking action $a\u0026rsquo;$ from state $s\u0026rsquo;$. This is summarized by the term: $\\gamma \\sum_a \\pi(a\u0026rsquo;,s\u0026rsquo;) q_\\pi(a\u0026rsquo;,s\u0026rsquo;)$\nPut together, we obtain the Bellman equation for the action value.\nOptimal policies and value functions: Comparing policies: One of the main reasons for introducing value functions is to be able to compare policies, and therefore search for an optimal policy: a policy for which the value is maximal in each state. Consider two policies $\\pi$ and $\\pi\u0026rsquo;$: $$\\pi \\geq \\pi\u0026rsquo; \\implies v_\\pi(s) \\geq v_{\\pi\u0026rsquo;}(s)\\text{ } \\forall s \\in \\mathcal{S}$$ $\\pi$ is superior or equal to $\\pi\u0026rsquo;$ if and only if the value of any state $s$ under $\\pi$ is superior or equal to the value of state $s$ under $\\pi\u0026rsquo;$. A policy that is superior or equal to all other policies is called an optimal policy and is noted $\\pi_{\\star}$. In fact, there is always at least one optimal policy, therefore $\\pi_{\\star}$ refers to any optimal policy.\nWhy is that ? Imagine two policies $\\pi_1$ and $\\pi_2$, whith $\\pi_1\u0026gt;\\pi_2$ in a certain state A and $\\pi_1\u0026lt;\\pi_2$ in another state B. It is possible to create a policy $\\pi_3$ as a combination of both previous policies so that $\\pi_3$ always follows the policy with the highest value in the current state. Therefore, $\\pi_3$ will necessarily have a value superior or equal to both policies in every state. There exists a formal proof to this problem, however this argument helps us understand that we will not encounter situations where we have to sacrifice value in one state to achieve value in another, and therefore there exist a policy that is best in every state.\nOptimal value functions: Using an optimal policy $\\pi_{\\star}$, we can define the optimal value functions $v_\\star$ and $q_\\star$. The state value function for an optimal policy has the greatest value in each state, it can be defined as follows: $$v_{\\pi \\star}(s) = E_{\\pi \\star}[G_t|S_t=s] = \\max_{\\pi} v_\\pi(s) \\text{ for all } s \\in \\mathcal S$$ In other words $v_{\\pi \\star}(s)$ is the equal to the maximum value over all policies.\nOptimal policies also share the same action value function, given by: $$q_{\\pi\\star}(s,a) = \\max_\\pi q_\\pi(s,a) \\text{ for all } s \\in \\mathcal S \\text{ and } a \\in \\mathcal A$$\nBellman optimality equations: The Bellman equations for optimal state and action value functions are called the Bellman optimality equations. Let\u0026rsquo;s recall the Bellman equation for the state value function and substitute the policy $\\pi$ by an optimal policy $\\pi_\\star$: $$v_\\star(s) = \\sum_a\\pi_\\star(a|s)\\sum_{s\u0026rsquo;,r}p[s\u0026rsquo;,r|s,a](r + \\gamma v_{\\star}(s\u0026rsquo;))$$ Now, as $\\pi_\\star$ is an optimal policy, we can rewrite the equation in a form that doesn\u0026rsquo;t reference the policy. Indeed, the optimal policy select the best action in every state by assigning probability 1 to the action yielding maximal value and 0 to the others. This can be expressed by replacing $\\pi_\\star$ by the maximum over all actions: $$v_\\star(s) = \\max_a\\sum_{s\u0026rsquo;,r}p[s\u0026rsquo;,r|s,a](r + \\gamma v_\\star(s\u0026rsquo;))$$\nWe can apply the same reasoning to $q_\\star$: $$q_\\star(s) = \\sum_{s\u0026rsquo;,r} p[s\u0026rsquo;,r|s,a](r + \\gamma \\sum_{a\u0026rsquo;} \\pi_\\star(a\u0026rsquo;,s\u0026rsquo;) q_\\star(a\u0026rsquo;,s\u0026rsquo;))$$ $$q_\\star(s) = \\sum_{s\u0026rsquo;,r} p[s\u0026rsquo;,r|s,a](r + \\gamma \\max_{a\u0026rsquo;} q_\\star(a\u0026rsquo;,s\u0026rsquo;))$$\nFinding an optimal policy from an optimal value function: To conclude this chapter, we\u0026rsquo;ll derive a formula for the optimal policy based on optimal value functions.\nFrom $v_\\star$: Having access to the dynamics function $p$ and the optimal state value function $v_\\star$ makes it easy to determine $\\pi_\\star$. Indeed, for every state, we\u0026rsquo;ll compute the term $\\sum_{s\u0026rsquo;,r}p[s\u0026rsquo;,r|s,a](r + \\gamma v_\\star(s\u0026rsquo;))$ for each action. This can be seen as a one-step lookahead, as depicted on the backup diagram. For some actions, this term will reach a maximum, a deterministic policy selecting these actions for each state will necessarily be optimal. Now let\u0026rsquo;s see how to derive $\\pi_\\star$ from $v_\\star$: $$v_\\star(s) = \\max_a\\sum_{s\u0026rsquo;,r}p[s\u0026rsquo;,r|s,a](r + \\gamma v_\\star(s\u0026rsquo;))$$ Recall that $v_\\star$ simply returns the maximal value for state $s$, the optimal policy picks the action associated with this maximum value. Therefore, we just have to replace the $\\max$ operator by $argmax$: $$\\pi_\\star(s) = \\text{arg}\\max_a\\sum_{s\u0026rsquo;,r}p[s\u0026rsquo;,r|s,a](r + \\gamma v_\\star(s\u0026rsquo;))$$\nFrom $q_\\star$: If instead we have access to $q_\\star$, deriving $\\pi_\\star$ is even easier. We only have to select the action $a$ that maximizes $q_\\star(s,a)$.\n$$\\pi_\\star(s) = \\text{arg}\\max_a \\text{ }q_\\star(s,a)$$\nWrapping up: In conclusion, the Bellman equation is a fundamental concept in the field of Reinforcement Learning, enabling us to compute the optimal value function of an agent in a Markov Decision Process. Through the use of the Bellman equation, we can efficiently solve complex decision-making problems by breaking them down into smaller sub-problems.\nIn the next article, we will dive deeper into the world of Dynamic Programming algorithms, a class of algorithms that utilizes the Bellman equation to solve problems in a systematic and efficient manner. We will explore various dynamic programming algorithms such as Value Iteration and Policy Iteration, and see how they can be used to solve larger and more complex decision-making problems.\n","date":"2023-04-29T00:00:00Z","image":"https://machine-learning-blog.vercel.app/p/bellman_equation/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1565692_120x120_fill_q75_box_smart1.jpg","permalink":"https://machine-learning-blog.vercel.app/p/bellman_equation/","title":"The Bellman equation"},{"content":"Credits: Intuitively Understanding the KL Divergence, Wikipedia\nIn recent years, reinforcement learning (RL) has emerged as a powerful paradigm for solving complex decision-making problems. One of the fundamental challenges in RL is to accurately estimate the value of different actions in a given state. KL divergence, a measure of how one probability distribution differs from another, has become an essential tool for addressing this challenge. KL divergence is widely used in machine learning, particularly in deep learning and probabilistic modeling, to compare the difference between two probability distributions. In the context of RL, KL divergence plays a crucial role in many algorithms, such as policy optimization and value function approximation. In this article, we will explore the importance of KL divergence in RL and machine learning in general and discuss its applications in solving a wide range of problems.\nDefinition The Kullback-Leibler divergence is a statistical distance measuring how different a probability distribution is from a reference distribution. We usually consider two distributions:\n$P$ which serves as reference, it represents the data or observations $Q$ which represents a model or an approximation of $P$ The KL divergence is defined for both continuous and discrete probability distributions.\nDiscrete case For $P$ and $Q$ defined on the same sample space $\\mathcal{X}$, the KL divergence from $Q$ to $P$ is defined as: $$D_{KL}(P||Q) = \\sum_{x \\in \\mathcal{X} }P(x)log({P(x)\\over{Q(x)} })$$ One way to interpret this formula is to consider it as the expectation of the logarithmic difference between the probabilities $P$ and $Q$, where the expectation is taken using $P$ as reference.\nContinuous case For distributions $P$ and $Q$ of a continuous random variable, the KL divergence from $Q$ to $P$ is defined as the integral: $$D_{KL}(P||Q) = \\int_{-\\infty}^{\\infty}P(x)log({P(x)\\over{Q(x)} })dx$$\nIntuitive explanation As mentioned previously, the KL divergence is intended to measure the difference between probability distributions. In other words it measures how likely it is for the distribution $Q$ to generate samples from distribution $P$.\nLet\u0026rsquo;s take the simplest example of a Binomial distribution, the coin toss: Consider two coins, the first one is a fair coin, therefore $P(heads) = P(tails) = 0.5$\nHowever the second coin is biased, $P(heads)=p$ and $P(tails)=1-p$\nHow can we measure the distance between these two distributions ? Certainly if $p$ is close to 0.55, the distributions would be much more similar than if $p=0.95$. Indeed if $p=0.55$, then it would be easy to confuse the two distributions. We could measure this by comparing the probability of a specific sequence under both distributions.\nLet\u0026rsquo;s say we toss the first coin 10 times and obtain the following sequence: $$H,T,T,H,T,H,H,H,T,H$$ Comparing the likelihood of this sequence happening for the fair coin and the biased coin could boil down to computing: $${P(sequence | \\text{fair coin})\\over P(sequence | \\text{biased coin})}$$ To compute the likelihood of this sequence happening for both coins, lets define:\n$p_1 = P(head|\\text{fair coin})$ and $p_2 = P(tails|\\text{fair coin})$ $q_1 = P(head|\\text{biased coin})$ and $q_2 = P(tails|\\text{biased coin})$ Intuitively, the probability of observing the previous sequence would be: $$P(sequence|\\text{fair coin}) = p_1\\times p_2\\times p_2\\times p_1\\times p_2\\times p_1\\times p_1\\times p_1\\times p_2\\times p_1$$ A more elegant way to write this expression is obtained by raising both probabilities to the power $N_H$ and $N_T$ where $N_H$ is the number of heads and $N_T$ the number of tails. $$P(sequence|\\text{fair coin}) = p_1^{N_H} \\text{ } p_2^{N_T}$$ $$P(sequence|\\text{biased coin}) = q_1^{N_H} \\text{ } q_2^{N_T}$$\nTherefore, the ratio defined previously becomes: $${P(sequence | \\text{fair coin})\\over P(sequence | \\text{biased coin}) } = {p_1^{N_H} \\text{ } p_2^{N_T}\\over q_1^{N_H} \\text{ } q_2^{N_T} }$$ Believe it or not, the KL divergence is just around the corner ! Let\u0026rsquo;s normalize for sample size by raising the ratio to the power of $1/N$ and then take the log of the expression, we obtain: $$log({p_1^{N_H} \\text{ } p_2^{N_T}\\over q_1^{N_H} \\text{ } q_2^{N_T} })^{1\\over N}$$ Using the log properties, we obtain the following equivalences: $${1\\over N}log({p_1^{N_H} \\text{ } p_2^{N_T}\\over q_1^{N_H} \\text{ } q_2^{N_T} })$$ By breaking down multiplications and divisions: $${1\\over N}log\\text{ }p_1^{N_H} + {1\\over N}log\\text{ }p_2^{N_T} - {1\\over N}log\\text{ }q_1^{N_H} - {1\\over N}log\\text{ }q_2^{N_T}$$ We can once again drop down the powers: $${N_H\\over N}log\\text{ }p_1+ { {N_T}\\over N}log\\text{ }p_2 - { {N_H}\\over N}log\\text{ }q_1 - { {N_T}\\over N}log\\text{ }q_2$$\nNow, if the observations are generated by the fair coin (which we use as reference), then, as $N$ tends to infinity, the proportion of observed heads becomes $p_1$ and the proportion of observed tails becomes $p_2$.\nTherefore, in the limit we can say that ${N_H\\over N} = p_1$ and ${N_T\\over N} = p_2$\nFrom there, we can simplify the equation and finally get to the discrete definition of the KL divergence: $$p_1\\text{ }log\\text{ }p_1+ p_2\\text{ }log\\text{ }p_2 - p_1\\text{ }log\\text{ }q_1 - p_2\\text{ }log\\text{ }q_2$$ $$= p_1\\text{ }log\\text{ }{p_1\\over q_1} + p_2\\text{ }log{p_2 \\over q_2}$$ $$= \\sum p(x)log\\text{ }{p(x)\\over q(x)}$$ Please note that this proof also holds for more than two classes.\nTo remember While the KL divergence is a distance it is not a metric for the following reasons: It is not symmetric It doesn\u0026rsquo;t satisfy the triangle inequality Indeed the KL divergence is not symmetric, meaning that the KL divergence between two probability distributions P and Q is not necessarily equal to the KL divergence between Q and P. This is because the KL divergence measures the difference between two probability distributions in terms of how much information is lost when using Q to approximate P, and the amount of information lost depends on which distribution is used as the reference.\nIn other words, the KL divergence measures the extent to which one probability distribution differs from another, and this difference may be asymmetric depending on the specific context and the reference distribution used. For example, in some cases, one distribution may be a much better approximation of another than vice versa, resulting in different KL divergences when comparing the two distributions.\nThe KL Loss is equivalent to the Cross-Entropy Loss since both aim to minize distances between distributions ","date":"2023-05-01T00:00:00Z","image":"https://machine-learning-blog.vercel.app/p/kl_divergence/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1279211_120x120_fill_q75_box_smart1.jpg","permalink":"https://machine-learning-blog.vercel.app/p/kl_divergence/","title":"KL divergence"}]