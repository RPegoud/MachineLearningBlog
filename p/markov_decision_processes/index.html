<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Welcome to Hugo Theme Stack"><title>Markov Decision Processes</title><link rel=canonical href=https://machine-learning-blog.vercel.app/p/markov_decision_processes/><link rel=stylesheet href=/scss/style.min.abbd69b2908fdfcd5179898beaafd374514a86538d81639ddd2c58c06ae54e40.css><meta property="og:title" content="Markov Decision Processes"><meta property="og:description" content="Welcome to Hugo Theme Stack"><meta property="og:url" content="https://machine-learning-blog.vercel.app/p/markov_decision_processes/"><meta property="og:site_name" content="Ryan's ML Gym"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="Theory"><meta property="article:published_time" content="2023-04-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-03T00:00:00+00:00"><meta property="og:image" content="https://machine-learning-blog.vercel.app/p/markov_decision_processes/cover.jpg"><meta name=twitter:title content="Markov Decision Processes"><meta name=twitter:description content="Welcome to Hugo Theme Stack"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://machine-learning-blog.vercel.app/p/markov_decision_processes/cover.jpg"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hua2fcca72f1e2583e003df056155e3d9b_18418_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>ü§ñ</span></figure><div class=site-meta><h1 class=site-name><a href=/>Ryan's ML Gym</a></h1><h2 class=site-description>‚ÄúNo problem can withstand the assault of sustained thinking.‚Äù ‚Äî Voltaire</h2></div></header><ol class=social-menu><li><a href=https://github.com/RPegoud target=_blank title=GitHub rel=me><svg class="feather feather-github" fill="none" height="24" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://www.linkedin.com/in/ryan-p%c3%a9goud/ target=_blank title=Linkedin rel=me><svg class="feather feather-linkedin" fill="none" height="24" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect height="12" width="4" x="2" y="9"/><circle cx="4" cy="4" r="2"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#markov-state><em><strong>Markov State</strong></em></a></li><li><a href=#reward-and-reward-hypothesis><em><strong>Reward and reward hypothesis:</strong></em></a></li><li><a href=#return-and-episodes><em><strong>Return and episodes</strong></em></a></li><li><a href=#continuing-tasks-and-discounting-rewards><em><strong>Continuing tasks and discounting rewards</strong></em></a></li><li><a href=#wrapping-up><em><strong>Wrapping up</strong></em></a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/markov_decision_processes/><img src=/p/markov_decision_processes/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_209593_800x0_resize_q75_box.jpg srcset="/p/markov_decision_processes/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_209593_800x0_resize_q75_box.jpg 800w, /p/markov_decision_processes/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_209593_1600x0_resize_q75_box.jpg 1600w" width=800 height=534 loading=lazy alt="Featured image of post Markov Decision Processes"></a></div><div class=article-details><header class=article-category><a href=/categories/reinforcement-learning/ style=background-color:#2a9d8f;color:#fff>Reinforcement Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/markov_decision_processes/>Markov Decision Processes</a></h2><h3 class=article-subtitle>Welcome to Hugo Theme Stack</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Apr 03, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>7 minute read</time></div></footer></div></header><section class=article-content><p><code>Markov Decision Processes are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal.</code></p><p>You might have already wondered how Reinforcement Learning (RL) agents manage to reach super-human levels in games such as chess.
Just like in chess, where a player has to <em><strong>make decisions based on the current state of the board</strong></em> and the available moves, RL agents also face similar decision-making challenges in dynamic environments.</p><p>Markov Decision Processes (MDP) <strong>provide a mathematical framework</strong> to <strong>model such sequential decision-making problems</strong>, making them a fundamental tool in Reinforcement Learning.</p><h2 id=markov-state><em><strong>Markov State</strong></em></h2><p>To understand an RL problem, we need to define its key components. The main character in this scenario, responsible for learning and making decisions, is referred to as the <strong>agent</strong>. On the other side of the equation, we have the <strong>environment</strong>, which encompasses everything the agent interacts with.</p><p>These two, the agent and the environment, engage in an ongoing interaction. The agent takes actions, and the environment responds to these actions, presenting new situations to the agent. After each action of the agent, the environment returns a <strong>state</strong>, a snapshot of the environment at a given time, providing essential information for decision-making.</p><p>The totality of the state is <strong>not always revealed to the agent</strong>. In games like poker for example, the state could be represented as the cards held by all the players, the cards on the table and the order of cards in the deck. However, <strong>each player only has partial knowledge of the state</strong>. While their action modify the state of the environment, they only get an <strong>observation</strong> back from the environment. We call
these settings <strong>partially observable</strong>.</p><p>Inversely, in chess, each player can observe the state of the board fully.</p><p>Finally, the environment offers <strong>rewards</strong>, which are specific numerical values that the agent aims to maximize over time by making thoughtful choices in its actions.</p><center><img src="RL Loop.png"></center><p>In a <em>finite</em> MDP, the sets of states, actions and rewards $(\mathcal{S, A} \text{ and } \mathcal{R})$ have a finite number of elements.
In this case, the random variables $R_t$ and $S_t$ have discrete probability distributions dependent only on the preceding state and action.
We then note the probability of transitioning to a new state $s&rsquo;$ while receiving a reward $r$ based on the current state $s$ and the action $a$ taken by the agent:</p><p>$$p(s&rsquo;,r|s,a) = Pr(S_t=s&rsquo;, R_t=r | S_{t-1}=s, A_{t-1}=a)$$</p><p>For every value of these random variables, $s&rsquo; \in \mathcal S$ and $r \in \mathcal R$, there is a probability of those values occuring at time $t$, given particular values of the preceding state and action. The function $p$ defines the <strong>dynamics</strong> of the MDP. The dynamics function $p : \mathcal S ~X~ \mathcal R ~X~ \mathcal S ~X~ \mathcal A ‚Üí [0,1]$ is a <strong>probability distribution</strong> (as indicated by the &lsquo;$|$&rsquo;), therefore the sum of probabilites of all states $s&rsquo;$ and rewards $r$ given a state $s$ and an action $a$ equals 1. $$\sum_{s&rsquo; \in \mathcal S}\sum_{r \in \mathcal R}p(s&rsquo;,r|s,a)=1$$ In an MDP, the probability given by $p$ <strong>completely</strong> characterizes the environment dynamics.</p><p>Indeed, an environment having the <strong>Markov property</strong> means that the probability of each possible value for $\mathcal S_t$ and $\mathcal R_t$ depends <strong>only on the immediately preceding state and action</strong>, $\mathcal S_{t-1}$ and $\mathcal A_{t-1}$, and not at all on earlier states and actions.</p><p>This is best viewed as a restriction not on the decision process, but on the state. The <strong>state must include information about all aspects of the past agent‚Äìenvironment interactions</strong> that make a difference for the future.
This is a crucial assumption in Reinforcement Learning as it simplifies the learning problem by allowing the agent to ignore the history of past states and actions and focus solely on the current state.</p><p>A chess game is a good example of an MDP. We can define the environment&rsquo;s state as the position of all the pieces on the board. In this setting, the <strong>game history</strong> (sequence of moves played to get to the current position) <strong>is not useful to predict the best possible move</strong>. Therefore the environment is said to be Markov.</p><center><img src="Markov State.png"></center><h2 id=reward-and-reward-hypothesis><em><strong>Reward and reward hypothesis:</strong></em></h2><p>At each timestep, the agent receives a <em><strong>reward</strong></em> $R_t \in \mathbb{R}$ defining its purpose. The goal of the agent is to maximize the total amount of rewards i.e. the cumulative reward. This idea is illustrated by the <em><strong>reward hypothesis</strong></em>:</p><blockquote><p><em><strong>&ldquo;All of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)&rdquo;</strong></em></p></blockquote><p>To complete the chess example, we could define the reward as being <strong>positive</strong> when the agent <strong>wins</strong> the game and <strong>negative</strong> when it <strong>loses</strong> or <strong>draws</strong>. One could argue that capturing pieces should generate positive rewards, however it is possible to lose or draw the game after capturing almost all the opponent&rsquo;s pieces.</p><p>It is important to understand that rewards are used to <strong>set the goal to achieve and not the way to achieve it</strong> which is for the agent to figure out.</p><h2 id=return-and-episodes><em><strong>Return and episodes</strong></em></h2><p>Now that we have defined the notion of reward, we are interested in maximizing this reward over time. For this we need to define a new variable, the <strong>return</strong>.</p><p>The return $G_t$ is the <strong>sum of future rewards</strong> at timestep $t$.
$$G_t = R_{t+1}+R_{t+2}+R_{t+3}+&mldr;=\sum_{k=0}^\infty R_{t+k+1}$$
Importantly, the return is a <strong>random variable</strong>, as the dynamics of the MDP can be <strong>stochastic</strong> (i.e. involve randomness). In other words, the same actions can lead to different rewards if the environment dynamics are random.
Therefore we want to consider and maximize the <strong>expected return</strong>, the expected sum of future rewards:</p><p>$$\mathbb{E}[G_t] = \mathbb{E}[\sum_{k=0}^\infty R_{t+k+1}]$$</p><p>For this definition to make sense, the sum of rewards has to be <strong>finite</strong>. What happens if we want the agent to perform a task continuously, i.e. if the number of timesteps is infinite ?</p><p>We must distinguish two cases:</p><ul><li><strong>Episodic MDPs</strong>: the MDP can be naturally decomposed in <strong>episodes</strong>, a finite sequence of actions that end on a <strong>terminal state</strong>. Each episode starts in the <strong>same configuration</strong> and is <strong>independent from previous episodes</strong>.<ul><li>A game of chess is again a good example, <strong>each game starts in the same setting regardless of previous games</strong>. A game ends by a draw or by checkmate (terminal states). When the game ends we can reset the board and start anew.</li></ul></li><li><strong>Continuous MDPs</strong>: here the number of timesteps in <strong>infinite</strong>, the MDP goes on continually<ul><li>An example of continuous MDP could be controlling a dam to optimize the energy production depending on the predicted demand. The dam being permanently active, there is no terminal state (if we leave out special events such as maintenance).</li></ul></li></ul><h2 id=continuing-tasks-and-discounting-rewards><em><strong>Continuing tasks and discounting rewards</strong></em></h2><p>One might wonder, how can the expected return be finite when the MDP goes on forever ?</p><p>For continuous MDPs, the return is defined as the <strong>discounted</strong> sum of future rewards. The discount factor $\gamma \in [0,1)$ makes sure that <em><strong>rewards far in the future receive a lower weight</strong></em>:
$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + &mldr; = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$$</p><p>This acknowledges that rewards obtained in the future are generally considered less valuable than immediate rewards. This concept is crucial when dealing with tasks that have uncertain, long-term consequences.</p><p>A high discount factor values immediate rewards more, while a low factor assigns similar importance to rewards regardless of when they are received.</p><center><img src="Discounted Reward.png"></center><p>We can prove that the discounted return is finite by defining $R_{max}$ as the highest reward the agent can receive, therefore:
$$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \leq \sum_{k=0}^\infty \gamma^k R_{max}$$
As $R_{max}$ is constant, we can write:
$$\sum_{k=0}^\infty \gamma^k R_{max} = R_{max} \sum_{k=0}^\infty \gamma^k$$
As $\sum_{k=0}^\infty \gamma^k$ is a geometric series converging for $\gamma \lt 1$ we can conclude that:
$$R_{max} \sum_{k=0}^\infty \gamma^k = R_{max} \times { {1}\over{1-\gamma} } $$
In conclusion, $G_t$ has a finite upper bound and is therefore finite:
$$G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} \leq R_{max} \times { {1}\over{1-\gamma} }$$</p><h2 id=wrapping-up><em><strong>Wrapping up</strong></em></h2><p>There we have it ! In this article, we learned about <em><strong>MDPs</strong></em>, <em><strong>rewards</strong></em> and how to define the <em><strong>return for episodic and continuing tasks</strong></em>. In the next article, we&rsquo;ll see how we can <em><strong>solve</strong></em> an MDP to find the best possible action in each state by introducing the notions of <em><strong>policy</strong></em>, <em><strong>value functions</strong></em> and last but not least, the <a href=../bellman_equation/>Bellman Equation<a></p></section><footer class=article-footer><section class=article-tags><a href=/tags/theory/>Theory</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/bellman_equation/><div class=article-image><img src=/p/bellman_equation/cover.4958b8e498fbb30cca87ffee949bbfef_hu3d03a01dcc18bc5be0e67db3d8d209a6_1565692_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post The Bellman equation" data-key=bellman_equation data-hash="md5-SVi45Jj7swzKh//ulJu/7w=="></div><div class=article-details><h2 class=article-title>The Bellman equation</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2023 Ryan's ML Gym</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.20.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>