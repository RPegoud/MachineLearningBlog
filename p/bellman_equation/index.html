<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="The fundamental equation of RL"><title>The Bellman equation</title><link rel=canonical href=https://machine-learning-blog.vercel.app/p/bellman_equation/><link rel=stylesheet href=/scss/style.min.abbd69b2908fdfcd5179898beaafd374514a86538d81639ddd2c58c06ae54e40.css><meta property="og:title" content="The Bellman equation"><meta property="og:description" content="The fundamental equation of RL"><meta property="og:url" content="https://machine-learning-blog.vercel.app/p/bellman_equation/"><meta property="og:site_name" content="Ryan's ML Gym"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="Theory"><meta property="article:published_time" content="2023-04-29T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-29T00:00:00+00:00"><meta property="og:image" content="https://machine-learning-blog.vercel.app/p/bellman_equation/cover.jpg"><meta name=twitter:title content="The Bellman equation"><meta name=twitter:description content="The fundamental equation of RL"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://machine-learning-blog.vercel.app/p/bellman_equation/cover.jpg"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hua2fcca72f1e2583e003df056155e3d9b_18418_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>ü§ñ</span></figure><div class=site-meta><h1 class=site-name><a href=/>Ryan's ML Gym</a></h1><h2 class=site-description>‚ÄúNo problem can withstand the assault of sustained thinking.‚Äù ‚Äî Voltaire</h2></div></header><ol class=social-menu><li><a href=https://github.com/RPegoud target=_blank title=GitHub rel=me><svg class="feather feather-github" fill="none" height="24" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://www.linkedin.com/in/ryan-p%c3%a9goud/ target=_blank title=Linkedin rel=me><svg class="feather feather-linkedin" fill="none" height="24" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect height="12" width="4" x="2" y="9"/><circle cx="4" cy="4" r="2"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#policy><em><strong>Policy:</strong></em></a></li><li><a href=#value-functions><em><strong>Value functions:</strong></em></a><ol><li><a href=#state-value-function><em><strong>State value function:</strong></em></a></li><li><a href=#ction-value-function><em><strong>ction value function:</strong></em></a></li></ol></li><li><a href=#bellman-equations><em><strong>Bellman equations:</strong></em></a><ol><li><a href=#the-v-function><em><strong>The V function:</strong></em></a></li><li><a href=#the-q-function><em><strong>The Q function:</strong></em></a></li></ol></li><li><a href=#optimal-policies-and-value-functions><em><strong>Optimal policies and value functions:</strong></em></a><ol><li><a href=#comparing-policies><em><strong>Comparing policies:</strong></em></a></li><li><a href=#optimal-value-functions><em><strong>Optimal value functions:</strong></em></a></li><li><a href=#bellman-optimality-equations><em><strong>Bellman optimality equations:</strong></em></a></li></ol></li><li><a href=#finding-an-optimal-policy-from-an-optimal-value-function><em><strong>Finding an optimal policy from an optimal value function:</strong></em></a><ol><li><a href=#from-v_star><em><strong>From $v_\star$:</strong></em></a></li><li><a href=#from-q_star><em><strong>From $q_\star$:</strong></em></a></li></ol></li><li><a href=#wrapping-up><em><strong>Wrapping up:</strong></em></a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/bellman_equation/><img src=/p/bellman_equation/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1565692_800x0_resize_q75_box.jpg srcset="/p/bellman_equation/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1565692_800x0_resize_q75_box.jpg 800w, /p/bellman_equation/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1565692_1600x0_resize_q75_box.jpg 1600w" width=800 height=533 loading=lazy alt="Featured image of post The Bellman equation"></a></div><div class=article-details><header class=article-category><a href=/categories/reinforcement-learning/ style=background-color:#2a9d8f;color:#fff>Reinforcement Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/bellman_equation/>The Bellman equation</a></h2><h3 class=article-subtitle>The fundamental equation of RL</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Apr 29, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>9 minute read</time></div></footer></div></header><section class=article-content><p>In the previous articles, we had a look at the general setting of an RL problem by understanding the importance of the Markov property, the notion of rewards and return. Now that the agent evolves in a well defined environment and can receive rewards, we want to find a way to solve the MDP, in other words to maximize the expected return.</p><h2 id=policy><em><strong>Policy:</strong></em></h2><p>To maximize the return, we need our agent to pick the best possible actions from every state. You might wonder, how does the agent select actions ?
The answer is pretty straightforward, the agent follows a <em><strong>policy</strong></em> $\pi$, which is a <strong>probability distribution over all actions in a given state</strong>. Simply put, the policy is a <strong>function that assigns a probability to each available action at a specific point in time</strong>. The probability of picking an action $a$ in state $s$ is given by:
$$\pi(a|s)$$
As $\pi$ is a probability distribution, the sum of all action probabilities is 1:
$$\sum_{a \in \mathcal{A(s)} }\pi(a|s) = 1$$
Finding the best policy is equivalent to maximizing the expected return, therefore, we need a way to determine how good it is to follow a specific policy.</p><h2 id=value-functions><em><strong>Value functions:</strong></em></h2><p>Value functions allow us to get an idea of how valuable it is to pursue a specific policy in a certain state. Here, we&rsquo;ll introduce the <em><strong>state value function</strong></em> and the <em><strong>action value function</strong></em>:</p><h3 id=state-value-function><em><strong>State value function:</strong></em></h3><p>The state value function helps to answer the questions:</p><p><em><strong>How good is it to be in state $s$ and to follow the policy $\pi$ thereafter ?</strong></em></p><p>In an MDP, the state value function is defined as follows:
$$v_\pi(s) = E_\pi[G_t|S_t=s]$$
As you can see, the state value function is the expected return given that we are in state $s$ and follow the policy $\pi$. We can use the definition of the return to obtain the following form:
$$v_\pi(s) = E_\pi[\sum_{k=0}^\infty \gamma^kR_{t+k+1}|S_t=s]$$</p><h3 id=ction-value-function><em><strong>ction value function:</strong></em></h3><p>On the other hand, the action value function answers :</p><p><em><strong>How good is it to be in state $s$, pick action $a$ and follow the policy $\pi$ thereafter ?</strong></em></p><p>In an MDP, the action value function is defined as follows:
$$q_\pi(s) = E_\pi[G_t|S_t=s, A_t = a]$$
This equation is highly similar to the state value function, the only difference is that, before following $\pi$, we pick an action $a$.
$$q_\pi(s) = E_\pi[\sum_{k=0}^\infty \gamma^kR_{t+k+1}|S_t=s, A_t=a]$$</p><h2 id=bellman-equations><em><strong>Bellman equations:</strong></em></h2><p>Now that we have briefly defined value functions, we will introduce the Bellman equations, that are directly derived from the state value and action value functions. These equations are used to link the value of each state, or state-action pair to the value of its possible successors.</p><h3 id=the-v-function><em><strong>The V function:</strong></em></h3><p>To solve an MDP, we need to evaluate the values of states based on the actions and subsequent states available. The Bellman equation allows us to define the state value function using recursion, which will come in handy and allow us to iterate through the state space.
Let&rsquo;s start from the definition of the state value function:
$$v_\pi(s) = E_\pi[G_t|S_t=s]$$
Using the recursive definition of $G_t$, we obtain:
$$v_\pi(s) = E_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s]$$
We&rsquo;ll now decompose the expectation as three sums, one over each available actions $a$, over each subsequent state $s&rsquo;$ and rewards $r$ (look at the following backup graph for a more visual explanation !):
$$v_\pi(s) = \sum_a\pi(a|s)\sum_{s&rsquo;,r}p[s&rsquo;,r|s,a](r + \gamma E_{\pi}[G_{t+1}|S_{t+1}=s&rsquo;)]$$
Here you might recognize that $E_{\pi}[G_{t+1}|S_{t+1}=s&rsquo;]$ is the value of the state s&rsquo;, therefore:
$$v_\pi(s) = \sum_a\pi(a|s)\sum_{s&rsquo;,r}p[s&rsquo;,r|s,a](r + \gamma v_\pi(s&rsquo;))$$
For all state $s \in \mathcal{S}$</p><p><em>Note that $\sum_{s&rsquo;,r}$ is a compact notation for $\sum_{s&rsquo;}\sum_r$</em></p><p><em><strong>Visual explanation:</strong></em></p><p>This might seem complicated, but no worries, let&rsquo;s look at a more visual explanation with this backup diagram. We start in a state $s$ and want to estimate the value of being in this state. From there we can pick different actions, each action has a probability of being picked depending on the policy $\pi$ (remember that the policy is a probability distribution). After picking an action, the environment will return a new state $s&rsquo;$ and a reward $r$, both are subject to the environment&rsquo;s dynamics $p$ which is also a probability distribution.
The Bellman equation states that the value of being in state $s$ is the sum of the rewards and subsequent state values we can get from picking an action $a$ while being in state $s$ : $$[r + \gamma v_\pi(s&rsquo;)]$$This sum is weighted by the probability of obtaining each action, reward and state triplet:
$$ \sum_a\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)$$
Putting it all together, we obtain the Bellman equation for the state value.</p><center><img src=state_value_function.jpg></center><h3 id=the-q-function><em><strong>The Q function:</strong></em></h3><p>In the same fashion, we can derive the Bellman version of the action-value function starting from the definition:
$$q_\pi(s) = E_\pi[G_t|S_t=s, A_t=a]$$
However this time, we start from a state $s$ <em><strong>and</strong></em> take an action $a$ (refer to the following backup diagram). As the action we chose is fixed, the policy is not yet intervening. Using the same recursive property og $G_t$, we obtain:
$$q_\pi(s) = \sum_{s&rsquo;,r} p[s&rsquo;,r|s,a](r + \gamma E_\pi[G_{t+1}|S_{t+1}=s&rsquo;)]$$
We now have the expected return for states $s&rsquo;$, however we are interested in the value of <em>state-action pairs</em>. We can therefore use the following transformation and introduce the new action $a&rsquo;$ and its probability defined by $\pi$: $$\gamma E_\pi[G_{t+1}|S_{t+1}=s&rsquo;] = \gamma \sum_{a&rsquo;} \pi(a&rsquo;|s&rsquo;)E_\pi[G_{t+1}|S_{t+1}=s&rsquo;, A_{t+1}=a&rsquo;]$$ We effectively transformed the state value of $s&rsquo;$ into the action value of $s&rsquo;$, following the definition of $q_{\pi}$, the whole expression becomes:
$$q_\pi(s) = \sum_{s&rsquo;,r} p[s&rsquo;,r|s,a](r + \gamma \sum_{a&rsquo;} \pi(a&rsquo;,s&rsquo;) q_\pi(a&rsquo;,s&rsquo;))$$</p><p><em><strong>Visual explanation:</strong></em></p><p>This time, we start from a state $s$ and pick an action $a$, from there we can transition to different states $s&rsquo;$ and receive a reward $r$ depending on the environment&rsquo;s dynamics. As we evaluate all the possible transitions, we get the term $\sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)$.
We then want to estimate the value of the new state $s&rsquo;$ we landed in. This value corresponds to the transition reward $r$ summed with the (discounted) expected return from state $s&rsquo;$.
The expected return from state $s&rsquo;$ can be seen as the weighted sum of the values of each state-action pairs, with the weight being the probability of picking action $a&rsquo;$ from state $s&rsquo;$. This is summarized by the term: $\gamma \sum_a \pi(a&rsquo;,s&rsquo;) q_\pi(a&rsquo;,s&rsquo;)$</p><p>Put together, we obtain the Bellman equation for the action value.</p><center><img src=action_value_function.jpg></center><h2 id=optimal-policies-and-value-functions><em><strong>Optimal policies and value functions:</strong></em></h2><h3 id=comparing-policies><em><strong>Comparing policies:</strong></em></h3><p>One of the main reasons for introducing value functions is to be able to compare policies, and therefore search for an optimal policy: a policy for which the value is maximal in each state.
Consider two policies $\pi$ and $\pi&rsquo;$:
$$\pi \geq \pi&rsquo; \implies v_\pi(s) \geq v_{\pi&rsquo;}(s)\text{ } \forall s \in \mathcal{S}$$
$\pi$ is superior or equal to $\pi&rsquo;$ if and only if the value of any state $s$ under $\pi$ is superior or equal to the value of state $s$ under $\pi&rsquo;$.
A policy that is superior or equal to all other policies is called an <em><strong>optimal policy</strong></em> and is noted $\pi_{\star}$.
In fact, there is always at least one optimal policy, therefore $\pi_{\star}$ refers to any optimal policy.</p><p>Why is that ? Imagine two policies $\pi_1$ and $\pi_2$, whith $\pi_1>\pi_2$ in a certain state A and $\pi_1&lt;\pi_2$ in another state B. It is possible to create a policy $\pi_3$ as a combination of both previous policies so that $\pi_3$ always follows the policy with the highest value in the current state. Therefore, $\pi_3$ will necessarily have a value superior or equal to both policies in every state.
There exists a formal proof to this problem, however this argument helps us understand that we will not <em><strong>encounter situations where we have to sacrifice value in one state to achieve value in another</strong></em>, and therefore there exist a policy that is best in every state.</p><h3 id=optimal-value-functions><em><strong>Optimal value functions:</strong></em></h3><p>Using an optimal policy $\pi_{\star}$, we can define the optimal value functions $v_\star$ and $q_\star$.
The state value function for an optimal policy has the greatest value in each state, it can be defined as follows:
$$v_{\pi \star}(s) = E_{\pi \star}[G_t|S_t=s] = \max_{\pi} v_\pi(s) \text{ for all } s \in \mathcal S$$
In other words $v_{\pi \star}(s)$ is the equal to the maximum value over all policies.</p><p>Optimal policies also share the same action value function, given by:
$$q_{\pi\star}(s,a) = \max_\pi q_\pi(s,a) \text{ for all } s \in \mathcal S \text{ and } a \in \mathcal A$$</p><h3 id=bellman-optimality-equations><em><strong>Bellman optimality equations:</strong></em></h3><p>The Bellman equations for optimal state and action value functions are called the Bellman optimality equations.
Let&rsquo;s recall the Bellman equation for the state value function and substitute the policy $\pi$ by an optimal policy $\pi_\star$:
$$v_\star(s) = \sum_a\pi_\star(a|s)\sum_{s&rsquo;,r}p[s&rsquo;,r|s,a](r + \gamma v_{\star}(s&rsquo;))$$
Now, as $\pi_\star$ is an optimal policy, we can rewrite the equation in a form that doesn&rsquo;t reference the policy. Indeed, the optimal policy select the best action in every state by assigning probability 1 to the action yielding maximal value and 0 to the others. This can be expressed by replacing $\pi_\star$ by the maximum over all actions:
$$v_\star(s) = \max_a\sum_{s&rsquo;,r}p[s&rsquo;,r|s,a](r + \gamma v_\star(s&rsquo;))$$</p><p>We can apply the same reasoning to $q_\star$:
$$q_\star(s) = \sum_{s&rsquo;,r} p[s&rsquo;,r|s,a](r + \gamma \sum_{a&rsquo;} \pi_\star(a&rsquo;,s&rsquo;) q_\star(a&rsquo;,s&rsquo;))$$
$$q_\star(s) = \sum_{s&rsquo;,r} p[s&rsquo;,r|s,a](r + \gamma \max_{a&rsquo;} q_\star(a&rsquo;,s&rsquo;))$$</p><h2 id=finding-an-optimal-policy-from-an-optimal-value-function><em><strong>Finding an optimal policy from an optimal value function:</strong></em></h2><p>To conclude this chapter, we&rsquo;ll derive a formula for the optimal policy based on optimal value functions.</p><h3 id=from-v_star><em><strong>From $v_\star$:</strong></em></h3><p>Having access to the dynamics function $p$ and the optimal state value function $v_\star$ makes it easy to determine $\pi_\star$. Indeed, for every state, we&rsquo;ll compute the term $\sum_{s&rsquo;,r}p[s&rsquo;,r|s,a](r + \gamma v_\star(s&rsquo;))$ for each action. This can be seen as a one-step lookahead, as depicted on the backup diagram.
For some actions, this term will reach a maximum, a deterministic policy selecting these actions for each state will necessarily be optimal.
Now let&rsquo;s see how to derive $\pi_\star$ from $v_\star$:
$$v_\star(s) = \max_a\sum_{s&rsquo;,r}p[s&rsquo;,r|s,a](r + \gamma v_\star(s&rsquo;))$$
Recall that $v_\star$ simply returns the maximal value for state $s$, the optimal policy picks the action associated with this maximum value. Therefore, we just have to replace the $\max$ operator by $argmax$:
$$\pi_\star(s) = \text{arg}\max_a\sum_{s&rsquo;,r}p[s&rsquo;,r|s,a](r + \gamma v_\star(s&rsquo;))$$</p><h3 id=from-q_star><em><strong>From $q_\star$:</strong></em></h3><p>If instead we have access to $q_\star$, deriving $\pi_\star$ is even easier. We only have to select the action $a$ that maximizes $q_\star(s,a)$.</p><p>$$\pi_\star(s) = \text{arg}\max_a \text{ }q_\star(s,a)$$</p><h2 id=wrapping-up><em><strong>Wrapping up:</strong></em></h2><p>In conclusion, the Bellman equation is a fundamental concept in the field of Reinforcement Learning, enabling us to <em><strong>compute the optimal value function of an agent in a Markov Decision Process</strong></em>. Through the use of the Bellman equation, we can efficiently solve complex decision-making problems by breaking them down into smaller sub-problems.</p><p>In the next article, we will dive deeper into the world of <a href=../>Dynamic Programming<a> algorithms, a class of algorithms that utilizes the Bellman equation to solve problems in a systematic and efficient manner. We will explore various dynamic programming algorithms such as Value Iteration and Policy Iteration, and see how they can be used to solve larger and more complex decision-making problems.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/theory/>Theory</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/markov_decision_processes/><div class=article-image><img src=/p/markov_decision_processes/cover.409e726624061149b0dc1a97f4fe0e27_hu3d03a01dcc18bc5be0e67db3d8d209a6_209593_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post Markov Decision Processes" data-key=markov_decision_processes data-hash="md5-QJ5yZiQGEUmw3BqX9P4OJw=="></div><div class=article-details><h2 class=article-title>Markov Decision Processes</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2023 Ryan's ML Gym</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.20.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>