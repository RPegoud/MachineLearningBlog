<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Measuring distances between probability distribution"><title>KL divergence</title><link rel=canonical href=https://machine-learning-blog.vercel.app/p/kl_divergence/><link rel=stylesheet href=/scss/style.min.abbd69b2908fdfcd5179898beaafd374514a86538d81639ddd2c58c06ae54e40.css><meta property="og:title" content="KL divergence"><meta property="og:description" content="Measuring distances between probability distribution"><meta property="og:url" content="https://machine-learning-blog.vercel.app/p/kl_divergence/"><meta property="og:site_name" content="Ryan's ML Gym"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="Theory"><meta property="article:published_time" content="2023-05-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-01T00:00:00+00:00"><meta property="og:image" content="https://machine-learning-blog.vercel.app/p/kl_divergence/cover.jpg"><meta name=twitter:title content="KL divergence"><meta name=twitter:description content="Measuring distances between probability distribution"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://machine-learning-blog.vercel.app/p/kl_divergence/cover.jpg"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hua2fcca72f1e2583e003df056155e3d9b_18418_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>ü§ñ</span></figure><div class=site-meta><h1 class=site-name><a href=/>Ryan's ML Gym</a></h1><h2 class=site-description>‚ÄúNo problem can withstand the assault of sustained thinking.‚Äù ‚Äî Voltaire</h2></div></header><ol class=social-menu><li><a href=https://github.com/RPegoud target=_blank title=GitHub rel=me><svg class="feather feather-github" fill="none" height="24" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li><a href=https://www.linkedin.com/in/ryan-p%c3%a9goud/ target=_blank title=Linkedin rel=me><svg class="feather feather-linkedin" fill="none" height="24" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect height="12" width="4" x="2" y="9"/><circle cx="4" cy="4" r="2"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#definition><em><strong>Definition</strong></em></a><ol><li><a href=#discrete-case><em><strong>Discrete case</strong></em></a></li><li><a href=#continuous-case><em><strong>Continuous case</strong></em></a></li></ol></li><li><a href=#intuitive-explanation><em><strong>Intuitive explanation</strong></em></a></li><li><a href=#to-remember><em><strong>To remember</strong></em></a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/kl_divergence/><img src=/p/kl_divergence/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1279211_800x0_resize_q75_box.jpg srcset="/p/kl_divergence/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1279211_800x0_resize_q75_box.jpg 800w, /p/kl_divergence/cover_hu3d03a01dcc18bc5be0e67db3d8d209a6_1279211_1600x0_resize_q75_box.jpg 1600w" width=800 height=534 loading=lazy alt="Featured image of post KL divergence"></a></div><div class=article-details><header class=article-category><a href=/categories/statistics/>Statistics</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/kl_divergence/>KL divergence</a></h2><h3 class=article-subtitle>Measuring distances between probability distribution</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>May 01, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>5 minute read</time></div></footer></div></header><section class=article-content><p><em><strong>Credits: <a class=link href="https://www.youtube.com/watch?v=SxGYPqCgJWM" target=_blank rel=noopener>Intuitively Understanding the KL Divergence</a>, <a class=link href=https://en.wikipedia.org/wiki/Kullback%e2%80%93Leibler_divergence target=_blank rel=noopener>Wikipedia</a></strong></em></p><p>In recent years, reinforcement learning (RL) has emerged as a powerful paradigm for solving complex decision-making problems. One of the fundamental challenges in RL is to accurately estimate the value of different actions in a given state.
KL divergence, <em><strong>a measure of how one probability distribution differs from another</strong></em>, has become an essential tool for addressing this challenge. KL divergence is widely used in machine learning, particularly in deep learning and probabilistic modeling, to compare the difference between two probability distributions.
In the context of RL, <em><strong>KL divergence plays a crucial role in many algorithms, such as policy optimization and value function approximation</strong></em>. In this article, we will explore the importance of KL divergence in RL and machine learning in general and discuss its applications in solving a wide range of problems.</p><h2 id=definition><em><strong>Definition</strong></em></h2><p>The <em><strong>Kullback-Leibler divergence</strong></em> is a statistical distance measuring <em><strong>how different a probability distribution is from a reference distribution.</strong></em>
We usually consider two distributions:</p><ul><li>$P$ which serves as reference, it represents the data or observations</li><li>$Q$ which represents a model or an approximation of $P$</li></ul><p>The KL divergence is defined for both <em><strong>continuous</strong></em> and <em><strong>discrete probability distributions</strong></em>.</p><h3 id=discrete-case><em><strong>Discrete case</strong></em></h3><p>For $P$ and $Q$ defined on the same sample space $\mathcal{X}$, the KL divergence from $Q$ to $P$ is defined as:
$$D_{KL}(P||Q) = \sum_{x \in \mathcal{X} }P(x)log({P(x)\over{Q(x)} })$$
One way to interpret this formula is to consider it as the <em><strong>expectation of the logarithmic difference between the probabilities $P$ and $Q$</strong></em>, where the expectation is taken using $P$ as reference.</p><h3 id=continuous-case><em><strong>Continuous case</strong></em></h3><p>For distributions $P$ and $Q$ of a continuous random variable, the KL divergence from $Q$ to $P$ is defined as the integral:
$$D_{KL}(P||Q) = \int_{-\infty}^{\infty}P(x)log({P(x)\over{Q(x)} })dx$$</p><h2 id=intuitive-explanation><em><strong>Intuitive explanation</strong></em></h2><p>As mentioned previously, the KL divergence is intended to measure the difference between probability distributions. In other words it measures how likely it is for the distribution $Q$ to generate samples from distribution $P$.</p><p>Let&rsquo;s take the simplest example of a Binomial distribution, the coin toss:
Consider two coins, the first one is a fair coin, therefore $P(heads) = P(tails) = 0.5$</p><p>However the second coin is biased, $P(heads)=p$ and $P(tails)=1-p$</p><p>How can we measure the distance between these two distributions ? Certainly if $p$ is close to 0.55, the distributions would be much more similar than if $p=0.95$.
Indeed if $p=0.55$, then it would be easy to <em><strong>confuse</strong></em> the two distributions. We could measure this by <em><strong>comparing the probability of a specific sequence under both distributions</strong></em>.</p><p>Let&rsquo;s say we toss the first coin 10 times and obtain the following sequence:
$$H,T,T,H,T,H,H,H,T,H$$
Comparing the likelihood of this sequence happening for the fair coin and the biased coin could boil down to computing:
$${P(sequence | \text{fair coin})\over P(sequence | \text{biased coin})}$$
To compute the likelihood of this sequence happening for both coins, lets define:</p><ul><li>$p_1 = P(head|\text{fair coin})$ and $p_2 = P(tails|\text{fair coin})$</li><li>$q_1 = P(head|\text{biased coin})$ and $q_2 = P(tails|\text{biased coin})$</li></ul><p>Intuitively, the probability of observing the previous sequence would be:
$$P(sequence|\text{fair coin}) = p_1\times p_2\times p_2\times p_1\times p_2\times p_1\times p_1\times p_1\times p_2\times p_1$$
A more elegant way to write this expression is obtained by raising both probabilities to the power $N_H$ and $N_T$ where $N_H$ is the number of heads and $N_T$ the number of tails.
$$P(sequence|\text{fair coin}) = p_1^{N_H} \text{ } p_2^{N_T}$$
$$P(sequence|\text{biased coin}) = q_1^{N_H} \text{ } q_2^{N_T}$$</p><p>Therefore, the ratio defined previously becomes:
$${P(sequence | \text{fair coin})\over P(sequence | \text{biased coin}) } = {p_1^{N_H} \text{ } p_2^{N_T}\over q_1^{N_H} \text{ } q_2^{N_T} }$$
Believe it or not, the KL divergence is just around the corner ! Let&rsquo;s normalize for sample size by raising the ratio to the power of $1/N$ and then take the log of the expression, we obtain:
$$log({p_1^{N_H} \text{ } p_2^{N_T}\over q_1^{N_H} \text{ } q_2^{N_T} })^{1\over N}$$
Using the log properties, we obtain the following equivalences:
$${1\over N}log({p_1^{N_H} \text{ } p_2^{N_T}\over q_1^{N_H} \text{ } q_2^{N_T} })$$ By breaking down multiplications and divisions:
$${1\over N}log\text{ }p_1^{N_H} + {1\over N}log\text{ }p_2^{N_T} - {1\over N}log\text{ }q_1^{N_H} - {1\over N}log\text{ }q_2^{N_T}$$
We can once again drop down the powers:
$${N_H\over N}log\text{ }p_1+ { {N_T}\over N}log\text{ }p_2 - { {N_H}\over N}log\text{ }q_1 - { {N_T}\over N}log\text{ }q_2$$</p><p>Now, if the observations are generated by the fair coin (which we use as reference), then, as $N$ tends to infinity, the proportion of observed heads becomes $p_1$ and the proportion of observed tails becomes $p_2$.</p><p>Therefore, in the limit we can say that ${N_H\over N} = p_1$ and ${N_T\over N} = p_2$</p><p>From there, we can simplify the equation and finally get to the discrete definition of the KL divergence:
$$p_1\text{ }log\text{ }p_1+ p_2\text{ }log\text{ }p_2 - p_1\text{ }log\text{ }q_1 - p_2\text{ }log\text{ }q_2$$
$$= p_1\text{ }log\text{ }{p_1\over q_1} + p_2\text{ }log{p_2 \over q_2}$$
$$= \sum p(x)log\text{ }{p(x)\over q(x)}$$
Please note that this proof also holds for more than two classes.</p><h2 id=to-remember><em><strong>To remember</strong></em></h2><ul><li>While the KL divergence is a distance it is <em><strong>not a metric</strong></em> for the following reasons:<ul><li>It is not symmetric</li><li>It doesn&rsquo;t satisfy the triangle inequality</li></ul></li></ul><p>Indeed the KL divergence is <em><strong>not symmetric</strong></em>, meaning that the KL divergence between two probability distributions P and Q is not necessarily equal to the KL divergence between Q and P. This is because the KL divergence measures the difference between two probability distributions in terms of <em><strong>how much information is lost when using Q to approximate P</strong></em>, and <em><strong>the amount of information lost depends on which distribution is used as the reference</strong></em>.</p><p>In other words, the KL divergence measures the extent to which one probability distribution differs from another, and this difference may be asymmetric depending on the specific context and the reference distribution used. For example, in some cases, one distribution may be a much better approximation of another than vice versa, resulting in different KL divergences when comparing the two distributions.</p><ul><li>The KL Loss is <em><strong>equivalent to the Cross-Entropy Loss</strong></em> since both aim to minize distances between distributions</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/theory/>Theory</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><footer class=site-footer><section class=copyright>&copy;
2023 Ryan's ML Gym</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.20.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>